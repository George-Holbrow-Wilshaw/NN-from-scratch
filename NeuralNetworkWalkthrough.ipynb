{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook will..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contructing the network framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import random\n",
    "\n",
    "class SequentialNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def compile(self, optimiser, loss_function, metrics):\n",
    "        net = NeuralNetwork(layers = self.layers, \n",
    "                            optimiser = optimiser, \n",
    "                            loss_function = loss_function, \n",
    "                            metrics = metrics)\n",
    "        return net\n",
    "\n",
    "    def summary(self):\n",
    "        for l in self.layers:\n",
    "            print('%s Input shape = %s  Output shape = %s' %(l.name, l.input_shape, l.output_shape))\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, optimiser, loss_function, metrics):\n",
    "        \n",
    "        self.input_shape = layers[0].input_shape\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_layers_hidden = len(layers) - 1\n",
    "        self.n_nodes_hidden = sum([l.n_nodes for l in layers[1:]])\n",
    "        self.n_nodes_input = self.input_shape[0]\n",
    "        self.input_layer = layers[0]\n",
    "        self.output_layer = layers[-1]\n",
    "        self.ouput_activation = self.layers[-1].activation_function\n",
    "\n",
    "        # Produce weights and activation caches \n",
    "        \n",
    "        self.activation_cache = []\n",
    "        self.bias_cache = []\n",
    "        self.weight_cache = []\n",
    "\n",
    "        for l in range(len(layers)):\n",
    "            current_layer = layers[l]\n",
    "            depth = l\n",
    "\n",
    "            self.activation_cache.append(np.zeros(shape = (current_layer.n_nodes)))\n",
    "\n",
    "            if (depth > 0):\n",
    "                prev_layer = layers[l - 1]\n",
    "                self.bias_cache.append(np.random.rand(current_layer.n_nodes))\n",
    "                self.weight_cache.append(np.random.rand(prev_layer.n_nodes, current_layer.n_nodes) / 10)\n",
    "\n",
    "        self.z_cache = self.activation_cache\n",
    "\n",
    "    def forward_propogate(self, training_data):\n",
    "\n",
    "        self.activation_cache[0] = training_data\n",
    "\n",
    "        for l in enumerate(self.layers[:-1]):\n",
    "            layer_n = l[0]\n",
    "            layer = l[1]\n",
    "            layer.call_layer(self.activation_cache, self.weight_cache, self.bias_cache, layer_n)\n",
    "        \n",
    "        self.activation_cache[-1] = self.ouput_activation(self.activation_cache[-1])\n",
    "\n",
    "    def back_propogate(self, labels, activation_cache, z_cache, loss_function):\n",
    "\n",
    "        # First we figure out the gradient loss function\n",
    "        # This is defined as dC/dW = dZ/dW * dAdZ * dCdA\n",
    "        weights_adjustment_cache = [None] * (self.n_layers - 1)\n",
    "        bias_adjustment_cache = [None] * (self.n_layers - 1)\n",
    "\n",
    "\n",
    "        if self.ouput_activation == softmax:\n",
    "            deltas = (activation_cache[-1] - labels).T\n",
    "        else:\n",
    "            deltas = ((activation_cache[-1] - labels)[0] \\\n",
    "                * self.ouput_activation(z_cache[-1], prime = True)[0])\\\n",
    "                .reshape(self.layers[-1].n_nodes, 1)\n",
    "\n",
    "        dZdW = activation_cache[- 2]\n",
    "\n",
    "        dLdW = np.dot(deltas, dZdW)\n",
    "        dLdB = deltas\n",
    "\n",
    "        weights_adjustment_cache[-1] = (dLdW)\n",
    "        bias_adjustment_cache[-1] = (dLdB)\n",
    "        \n",
    "        for i in range(self.n_layers - 1, 1, -1):\n",
    "\n",
    "            layer = self.layers[i - 1]\n",
    "            deltas = np.dot(self.weight_cache[i - 1], deltas) \\\n",
    "                * layer.activation_function(z_cache[i - 1], prime = True).T\n",
    "            dZdW = activation_cache[i - 2]\n",
    "            dLdW = np.dot(deltas, dZdW)\n",
    "            dLdB = deltas\n",
    "\n",
    "            # Now we adjust our weights by a fraction of this gradient function (the gradient function tells us the direction and proportions we need to reduce the weights)\n",
    "            # This is in effect moving down an N dimensional surface \n",
    "            # This reduction will be done in another function, for the mean time we will store the amounts by which we need to reduce our weights\n",
    "\n",
    "            weights_adjustment_cache[i - 2] = dLdW\n",
    "            bias_adjustment_cache[i - 2] = dLdB\n",
    "\n",
    "        return weights_adjustment_cache, bias_adjustment_cache\n",
    "        \n",
    "    def validate(self, validation_data, validation_labels):\n",
    "\n",
    "        res = []\n",
    "        for x, y in zip(validation_data, validation_labels):\n",
    "            self.forward_propogate(x)\n",
    "            pred = np.argmax(self.activation_cache[-1])\n",
    "            actual = np.argmax(y)\n",
    "            r = True if pred == actual else False\n",
    "            res.append(r)\n",
    "        \n",
    "        validation_accuracy = sum(res) / len(validation_data)\n",
    "        \n",
    "        return validation_accuracy\n",
    "\n",
    "    def fit(self, training_data, labels, validation_split, metrics, n_epochs, learning_rate):\n",
    "        \n",
    "        self.n_training_samples = len(training_data)\n",
    "        \n",
    "        for _ in tqdm(range(n_epochs)):\n",
    "            \n",
    "            val_n = random.sample(range(self.n_training_samples - 1), int(validation_split * self.n_training_samples))\n",
    "            train_n = list(set(range(self.n_training_samples)) - set(val_n))\n",
    "            validation_data = training_data[val_n]\n",
    "            validation_labels = labels[val_n]\n",
    "            train_data = training_data[train_n]\n",
    "            train_labels = labels[train_n]\n",
    "\n",
    "            print('Training started: Epoch %d' %(_ + 1))\n",
    "            epoch_results = []\n",
    "            for x in tqdm(enumerate(train_data), total= len(train_data)):\n",
    "                y = train_labels[x[0]]\n",
    "                self.forward_propogate([x[1]])\n",
    "                weight_adjustments, bias_adjustments = self.back_propogate(y, self.activation_cache, self.z_cache, categorical_loss_entropy)\n",
    "                self.weight_cache = [w - w_adj.T * learning_rate for w, w_adj in zip(self.weight_cache, weight_adjustments)]\n",
    "                self.bias_cache = [b - b_adj.T * learning_rate for b, b_adj in zip(self.bias_cache, bias_adjustments)]\n",
    "                prediction = np.argmax(self.activation_cache[-1])\n",
    "                decoded_label = np.argmax(y)\n",
    "                res = True if prediction == decoded_label else False\n",
    "\n",
    "                epoch_results.append(res)\n",
    "            epoch_accuracy = sum(epoch_results) / float(len(train_data))\n",
    "\n",
    "            validation_accuracy = self.validate(validation_data, validation_labels)\n",
    "            print('Epoch training accuracy was %f \\n' %(epoch_accuracy))\n",
    "            print('Epoch validation accuracy was %f \\n' %(validation_accuracy))\n",
    "        return self\n",
    "\n",
    "    def predict(self, data):\n",
    "        prediction_weights = []\n",
    "        for sample in data:\n",
    "            self.forward_propogate(sample)\n",
    "            prediction_weights.append(self.activation_cache[-1])\n",
    "        return prediction_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the layer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FullyConnectedLayer:\n",
    "\n",
    "    def __init__(self, n_nodes, input_shape, activation_function):\n",
    "        self.name = 'FullyConnectedLayer'\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = 3\n",
    "        self.n_nodes = n_nodes\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def call_layer(self, data, weights, bias, layer_number = 0):\n",
    "        layer_weights = weights[layer_number]\n",
    "        layer_bias = bias[layer_number]\n",
    "        layer_data = data[layer_number]\n",
    "\n",
    "        z = np.dot(layer_data, layer_weights) + layer_bias\n",
    "        a = self.activation_function(z)\n",
    "\n",
    "        data[layer_number + 1] = a \n",
    "        \n",
    "class ConvolutionLayer:\n",
    "\n",
    "    def __init__(self, n_filters, kernel_size, input_shape, activation_function):\n",
    "        self.name = 'ConvolutionLayer'\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = 'tbd'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_filters = n_filters\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def call_layer(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss functions and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, prime = False):\n",
    "    if not prime:\n",
    "        s = 1 / (1 + (np.exp(x) ** - 1))\n",
    "    else:\n",
    "        s = sigmoid(x) * (1 - sigmoid(x))\n",
    "    return s\n",
    "\n",
    "\n",
    "def relu(x, prime = False):\n",
    "    if not prime:\n",
    "        r = np.maximum(0, x)\n",
    "        return r\n",
    "    else:\n",
    "        r = np.array([[1 if xi > 0 else 0 for xi in x[0]]])\n",
    "        return r\n",
    "\n",
    "def softmax(x, prime = False):\n",
    "    s = np.exp(x - np.max(x))\n",
    "    s = s / s.sum()\n",
    "    if prime:\n",
    "        s = s.reshape(-1,1)\n",
    "        s = np.diagflat(s) - np.dot(s, s.T)\n",
    "    return s\n",
    "\n",
    "def categorical_loss_entropy(actual, target):\n",
    "    # Takes np.arrays as scalars and targets\n",
    "    loss = -(target - np.log(actual))\n",
    "    return(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on MNDIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and restructing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "training_data = train_X[:50000, :, :].reshape(50000, 784) / 255.0\n",
    "train_y = train_y[:50000]\n",
    "\n",
    "test_data = test_X[:10000, :, :].reshape(10000, 784) / 255.0\n",
    "test_labels = test_y[:10000]\n",
    "\n",
    "\n",
    "# We need to one-hot encode our training labels\n",
    "training_labels = np.zeros((train_y.shape[0], train_y.max()+1), dtype=np.float32)\n",
    "training_labels[np.arange(train_y.shape[0]), train_y] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random_sample = random.sample(range(1, 50000), 5)\n",
    "\n",
    "for j, i in enumerate(random_sample):\n",
    "    im = training_data[i].reshape(28, 28)\n",
    "    plt.subplot(1, 5, j + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(im, cmap = 'Greys')\n",
    "    plt.subplots_adjust(wspace = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f877b5c8c4854c49b0a6284b7a171696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started: Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354b2e93ceeb4de4840e9e10c897226d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.530543 \n",
      "\n",
      "Epoch validation accuracy was 0.802667 \n",
      "\n",
      "Training started: Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a81a32e3dd74ca39647888b5a2713a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.835000 \n",
      "\n",
      "Epoch validation accuracy was 0.868467 \n",
      "\n",
      "Training started: Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccb610ec3674b0580a54d18076a891d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.873314 \n",
      "\n",
      "Epoch validation accuracy was 0.874067 \n",
      "\n",
      "Training started: Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896c1cec535a492a830c184be1f552ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.890657 \n",
      "\n",
      "Epoch validation accuracy was 0.881533 \n",
      "\n",
      "Training started: Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84e144503094365a3ed9cf12caf16d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.901229 \n",
      "\n",
      "Epoch validation accuracy was 0.903667 \n",
      "\n",
      "Training started: Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74ef519d11a45b8856a1f0e3cbf8c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.909714 \n",
      "\n",
      "Epoch validation accuracy was 0.911600 \n",
      "\n",
      "Training started: Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd7be3292064aa78ad4bd3bb0a595c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.917886 \n",
      "\n",
      "Epoch validation accuracy was 0.903800 \n",
      "\n",
      "Training started: Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e738505ded437e8c925639b290fad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.922143 \n",
      "\n",
      "Epoch validation accuracy was 0.916867 \n",
      "\n",
      "Training started: Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3ddc16122648318f45a7a1b4009128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.930343 \n",
      "\n",
      "Epoch validation accuracy was 0.909667 \n",
      "\n",
      "Training started: Epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953bb86d9e964df6a2bc6967626fcc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.934200 \n",
      "\n",
      "Epoch validation accuracy was 0.914533 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = SequentialNetwork([\n",
    "    FullyConnectedLayer(784, input_shape = (7, ), activation_function =  relu),\n",
    "    FullyConnectedLayer(120, input_shape = (3, ), activation_function =  relu),\n",
    "    FullyConnectedLayer(28, input_shape = (3, ), activation_function =  relu),\n",
    "    FullyConnectedLayer(10, input_shape = (3, ), activation_function =  softmax)\n",
    "])\n",
    "\n",
    "net = net.compile(optimiser = 'SGD',\n",
    "           loss_function = '',\n",
    "           metrics = '')\n",
    "\n",
    "\n",
    "model = net.fit(training_data = training_data,  \n",
    "        labels = training_labels, \n",
    "        validation_split = 0.3, \n",
    "        metrics='bel', \n",
    "        n_epochs = 10, \n",
    "        learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.918100\n",
      "Predicted labels: [2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4]\n",
      "Actual labels: [2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "test_d = test_data\n",
    "test_l = test_y\n",
    "\n",
    "prediction_weights = model.predict(test_d)\n",
    "prediction_classes = [np.argmax(p) for p in prediction_weights]\n",
    "res = [True if pred == actual else False for pred, actual in zip(prediction_classes, test_l)]\n",
    "accuracy = sum(res) / len(test_d)\n",
    "print('Accuracy on test set is %f'  %(accuracy))\n",
    "\n",
    "\n",
    "print('Predicted labels: ' + str(prediction_classes[1:20]))\n",
    "print('Actual labels: ' + str(test_l[1:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
