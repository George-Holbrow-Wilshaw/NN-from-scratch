{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook will..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contructing the network framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import random\n",
    "\n",
    "class SequentialNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def compile(self, optimiser, loss_function, metrics):\n",
    "        net = NeuralNetwork(layers = self.layers, \n",
    "                            optimiser = optimiser, \n",
    "                            loss_function = loss_function, \n",
    "                            metrics = metrics)\n",
    "        return net\n",
    "\n",
    "    def summary(self):\n",
    "        for l in self.layers:\n",
    "            print('%s Input shape = %s  Output shape = %s' %(l.name, l.input_shape, l.output_shape))\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, optimiser, loss_function, metrics):\n",
    "        \n",
    "        self.input_shape = layers[0].input_shape\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_layers_hidden = len(layers) - 1\n",
    "        self.n_nodes_hidden = sum([l.n_nodes for l in layers[1:]])\n",
    "        self.n_nodes_input = self.input_shape[0]\n",
    "        self.input_layer = layers[0]\n",
    "        self.output_layer = layers[-1]\n",
    "        self.ouput_activation = self.layers[-1].activation_function\n",
    "\n",
    "        # Produce weights and activation caches \n",
    "        \n",
    "        self.activation_cache = []\n",
    "        self.bias_cache = []\n",
    "        self.weight_cache = []\n",
    "\n",
    "        for l in range(len(layers)):\n",
    "            current_layer = layers[l]\n",
    "            depth = l\n",
    "\n",
    "            self.activation_cache.append(np.zeros(shape = (current_layer.n_nodes)))\n",
    "\n",
    "            if (depth > 0):\n",
    "                prev_layer = layers[l - 1]\n",
    "                self.bias_cache.append(np.random.rand(current_layer.n_nodes))\n",
    "                self.weight_cache.append(np.random.rand(prev_layer.n_nodes, current_layer.n_nodes) / 10)\n",
    "\n",
    "        self.z_cache = self.activation_cache\n",
    "\n",
    "    def forward_propogate(self, training_data):\n",
    "\n",
    "        self.activation_cache[0] = training_data\n",
    "\n",
    "        for l in enumerate(self.layers[:-1]):\n",
    "            layer_n = l[0]\n",
    "            layer = l[1]\n",
    "            layer.call_layer(self.activation_cache, self.weight_cache, self.bias_cache, layer_n)\n",
    "        \n",
    "        self.activation_cache[-1] = self.ouput_activation(self.activation_cache[-1])\n",
    "\n",
    "    def back_propogate(self, labels, activation_cache, z_cache, loss_function):\n",
    "\n",
    "        # First we figure out the gradient loss function\n",
    "        # This is defined as dC/dW = dZ/dW * dAdZ * dCdA\n",
    "        weights_adjustment_cache = [None] * (self.n_layers - 1)\n",
    "        bias_adjustment_cache = [None] * (self.n_layers - 1)\n",
    "\n",
    "\n",
    "        if self.ouput_activation == softmax:\n",
    "            deltas = (activation_cache[-1] - labels).T\n",
    "        else:\n",
    "            deltas = ((activation_cache[-1] - labels)[0] \\\n",
    "                * self.ouput_activation(z_cache[-1], prime = True)[0])\\\n",
    "                .reshape(self.layers[-1].n_nodes, 1)\n",
    "\n",
    "        dZdW = activation_cache[- 2]\n",
    "\n",
    "        dLdW = np.dot(deltas, dZdW)\n",
    "        dLdB = deltas\n",
    "\n",
    "        weights_adjustment_cache[-1] = (dLdW)\n",
    "        bias_adjustment_cache[-1] = (dLdB)\n",
    "        \n",
    "        for i in range(self.n_layers - 1, 1, -1):\n",
    "\n",
    "            layer = self.layers[i - 1]\n",
    "            deltas = np.dot(self.weight_cache[i - 1], deltas) \\\n",
    "                * layer.activation_function(z_cache[i - 1], prime = True).T\n",
    "            dZdW = activation_cache[i - 2]\n",
    "            dLdW = np.dot(deltas, dZdW)\n",
    "            dLdB = deltas\n",
    "\n",
    "            # Now we adjust our weights by a fraction of this gradient function (the gradient function tells us the direction and proportions we need to reduce the weights)\n",
    "            # This is in effect moving down an N dimensional surface \n",
    "            # This reduction will be done in another function, for the mean time we will store the amounts by which we need to reduce our weights\n",
    "\n",
    "            weights_adjustment_cache[i - 2] = dLdW\n",
    "            bias_adjustment_cache[i - 2] = dLdB\n",
    "\n",
    "        return weights_adjustment_cache, bias_adjustment_cache\n",
    "        \n",
    "    def validate(self, validation_data, validation_labels):\n",
    "\n",
    "        res = []\n",
    "        for x, y in zip(validation_data, validation_labels):\n",
    "            self.forward_propogate(x)\n",
    "            pred = np.argmax(self.activation_cache[-1])\n",
    "            actual = np.argmax(y)\n",
    "            r = True if pred == actual else False\n",
    "            res.append(r)\n",
    "        \n",
    "        validation_accuracy = sum(res) / len(validation_data)\n",
    "        \n",
    "        return validation_accuracy\n",
    "\n",
    "    def fit(self, training_data, labels, validation_split, metrics, n_epochs, learning_rate):\n",
    "        \n",
    "        self.n_training_samples = len(training_data)\n",
    "        \n",
    "        # Store intermediate results of activator and activation function outputs\n",
    "        self.a_hidden = np.zeros(shape = [self.n_training_samples, self.n_nodes_hidden, self.n_layers_hidden])\n",
    "        \n",
    "        for _ in tqdm(range(n_epochs)):\n",
    "            \n",
    "            val_n = random.sample(range(self.n_training_samples - 1), int(validation_split * self.n_training_samples))\n",
    "            train_n = list(set(range(self.n_training_samples)) - set(val_n))\n",
    "            validation_data = training_data[val_n]\n",
    "            validation_labels = labels[val_n]\n",
    "            train_data = training_data[train_n]\n",
    "            train_labels = labels[train_n]\n",
    "\n",
    "            print('Training started: Epoch %d' %(_ + 1))\n",
    "            epoch_results = []\n",
    "            for x in tqdm(enumerate(train_data), total= len(train_data)):\n",
    "                y = train_labels[x[0]]\n",
    "                self.forward_propogate([x[1]])\n",
    "                weight_adjustments, bias_adjustments = self.back_propogate(y, self.activation_cache, self.z_cache, categorical_loss_entropy)\n",
    "                self.weight_cache = [w - w_adj.T * learning_rate for w, w_adj in zip(self.weight_cache, weight_adjustments)]\n",
    "                self.bias_cache = [b - b_adj.T * learning_rate for b, b_adj in zip(self.bias_cache, bias_adjustments)]\n",
    "                prediction = np.argmax(self.activation_cache[-1])\n",
    "                decoded_label = np.argmax(y)\n",
    "                res = True if prediction == decoded_label else False\n",
    "\n",
    "                epoch_results.append(res)\n",
    "            epoch_accuracy = sum(epoch_results) / float(self.n_training_samples)\n",
    "\n",
    "            validation_accuracy = self.validate(validation_data, validation_labels)\n",
    "            print('Epoch training accuracy was %f \\n' %(epoch_accuracy))\n",
    "            print('Epoch validation accuracy was %f \\n' %(validation_accuracy))\n",
    "        return self\n",
    "\n",
    "    def predict(self, data):\n",
    "        prediction_weights = []\n",
    "        for sample in data:\n",
    "            self.forward_propogate(sample)\n",
    "            prediction_weights.append(self.activation_cache[-1])\n",
    "        return prediction_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the layer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FullyConnectedLayer:\n",
    "\n",
    "    def __init__(self, n_nodes, input_shape, activation_function):\n",
    "        self.name = 'FullyConnectedLayer'\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = 3\n",
    "        self.n_nodes = n_nodes\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def call_layer(self, data, weights, bias, layer_number = 0):\n",
    "        layer_weights = weights[layer_number]\n",
    "        layer_bias = bias[layer_number]\n",
    "        layer_data = data[layer_number]\n",
    "\n",
    "        z = np.dot(layer_data, layer_weights) + layer_bias\n",
    "        a = self.activation_function(z)\n",
    "\n",
    "        data[layer_number + 1] = a \n",
    "        \n",
    "class ConvolutionLayer:\n",
    "\n",
    "    def __init__(self, n_filters, kernel_size, input_shape, activation_function):\n",
    "        self.name = 'ConvolutionLayer'\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = 'tbd'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_filters = n_filters\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def call_layer(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss functions and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, prime = False):\n",
    "    if not prime:\n",
    "        s = 1 / (1 + (np.exp(x) ** - 1))\n",
    "    else:\n",
    "        s = sigmoid(x) * (1 - sigmoid(x))\n",
    "    return s\n",
    "\n",
    "\n",
    "def relu(x, prime = False):\n",
    "    if not prime:\n",
    "        r = np.maximum(0, x)\n",
    "        return r\n",
    "    else:\n",
    "        r = np.array([[1 if xi > 0 else 0 for xi in x[0]]])\n",
    "        return r\n",
    "\n",
    "def softmax(x, prime = False):\n",
    "    s = np.exp(x - np.max(x))\n",
    "    s = s / s.sum()\n",
    "    if prime:\n",
    "        s = s.reshape(-1,1)\n",
    "        s = np.diagflat(s) - np.dot(s, s.T)\n",
    "    return s\n",
    "\n",
    "def categorical_loss_entropy(actual, target):\n",
    "    # Takes np.arrays as scalars and targets\n",
    "    loss = -(target - np.log(actual))\n",
    "    return(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on MNDIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and restructing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "training_data = train_X[:50000, :, :].reshape(50000, 784) / 255.0\n",
    "train_y = train_y[:50000]\n",
    "\n",
    "test_data = test_X[:10000, :, :].reshape(10000, 784) / 255.0\n",
    "test_labels = test_y[:10000]\n",
    "\n",
    "\n",
    "# We need to one-hot encode our training labels\n",
    "training_labels = np.zeros((train_y.shape[0], train_y.max()+1), dtype=np.float32)\n",
    "training_labels[np.arange(train_y.shape[0]), train_y] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random_sample = random.sample(range(1, 50000), 5)\n",
    "\n",
    "for j, i in enumerate(random_sample):\n",
    "    im = training_data[i].reshape(28, 28)\n",
    "    plt.subplot(1, 5, j + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(im, cmap = 'Greys')\n",
    "    plt.subplots_adjust(wspace = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab0bc314e8e44f2bf07156df4e3c575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started: Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0548fe2e33ac4f31b8c91ebaf9b23448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.371240 \n",
      "\n",
      "Epoch validation accuracy was 0.808933 \n",
      "\n",
      "Training started: Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fef97b2d0b4da4bda8587b7e98b709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.583940 \n",
      "\n",
      "Epoch validation accuracy was 0.857000 \n",
      "\n",
      "Training started: Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386ec2be9d5740aba0f8ccd745a3294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.612180 \n",
      "\n",
      "Epoch validation accuracy was 0.875800 \n",
      "\n",
      "Training started: Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a52fe1f0c471d915d835d6a7600a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.624980 \n",
      "\n",
      "Epoch validation accuracy was 0.895200 \n",
      "\n",
      "Training started: Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2648d60a6fd4d11800e95cb179a77e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.632360 \n",
      "\n",
      "Epoch validation accuracy was 0.897133 \n",
      "\n",
      "Training started: Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b27f7181884bd182742edd05274e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.637640 \n",
      "\n",
      "Epoch validation accuracy was 0.906600 \n",
      "\n",
      "Training started: Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8c328070864c7faa032deef3427f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.641160 \n",
      "\n",
      "Epoch validation accuracy was 0.912200 \n",
      "\n",
      "Training started: Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96abc216e1f74cda8c485d31d63cd97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.647640 \n",
      "\n",
      "Epoch validation accuracy was 0.922067 \n",
      "\n",
      "Training started: Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b61f7d6b2b54624b1f72267908ff0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.649480 \n",
      "\n",
      "Epoch validation accuracy was 0.906733 \n",
      "\n",
      "Training started: Epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a748c15063c4b1e9509e5d366967d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training accuracy was 0.651220 \n",
      "\n",
      "Epoch validation accuracy was 0.928800 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = SequentialNetwork([\n",
    "    FullyConnectedLayer(784, input_shape = (7, ), activation_function =  relu),\n",
    "    FullyConnectedLayer(120, input_shape = (3, ), activation_function =  relu),\n",
    "    FullyConnectedLayer(28, input_shape = (3, ), activation_function =  relu),\n",
    "    FullyConnectedLayer(10, input_shape = (3, ), activation_function =  softmax)\n",
    "])\n",
    "\n",
    "net = net.compile(optimiser = 'SGD',\n",
    "           loss_function = '',\n",
    "           metrics = '')\n",
    "\n",
    "\n",
    "model = net.fit(training_data = training_data,  \n",
    "        labels = training_labels, \n",
    "        validation_split = 0.3, \n",
    "        metrics='bel', \n",
    "        n_epochs = 10, \n",
    "        learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.928900\n",
      "Predicted labels: [2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4]\n",
      "Actual labels: [2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "test_d = test_data\n",
    "test_l = test_y\n",
    "\n",
    "prediction_weights = model.predict(test_d)\n",
    "prediction_classes = [np.argmax(p) for p in prediction_weights]\n",
    "res = [True if pred == actual else False for pred, actual in zip(prediction_classes, test_l)]\n",
    "accuracy = sum(res) / len(test_d)\n",
    "print('Accuracy on test set is %f'  %(accuracy))\n",
    "\n",
    "\n",
    "print('Predicted labels: ' + str(prediction_classes[1:20]))\n",
    "print('Actual labels: ' + str(test_l[1:20]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
